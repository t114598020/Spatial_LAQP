{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a17bae3c",
   "metadata": {},
   "source": [
    "### Step1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d4d7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2049280 rows\n",
      "First 5 data:    timestamp  Global_active_power  Global_reactive_power  Voltage  \\\n",
      "0        0.0                4.216                  0.418   234.84   \n",
      "1       60.0                5.360                  0.436   233.63   \n",
      "2      120.0                5.374                  0.498   233.29   \n",
      "3      180.0                5.388                  0.502   233.74   \n",
      "4      240.0                3.666                  0.528   235.68   \n",
      "\n",
      "   Global_intensity  Sub_metering_1  Sub_metering_2  Sub_metering_3  \n",
      "0              18.4             0.0             1.0            17.0  \n",
      "1              23.0             0.0             1.0            16.0  \n",
      "2              23.0             0.0             2.0            17.0  \n",
      "3              23.0             0.0             1.0            17.0  \n",
      "4              15.8             0.0             1.0            17.0  \n",
      "Data size: 2049280\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to extracted TXT file\n",
    "file_path = 'household_power_consumption.txt'\n",
    "\n",
    "# Load with ; separator, handle '?' as NaN\n",
    "data = pd.read_csv(file_path, sep=';', na_values='?')\n",
    "\n",
    "# Combine Date and Time to datetime\n",
    "data['datetime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Numerical timestamp: seconds since the earliest date\n",
    "min_dt = data['datetime'].min()\n",
    "data['timestamp'] = (data['datetime'] - min_dt).dt.total_seconds()\n",
    "\n",
    "# Relevant columns (drop Date/Time/datetime, keep numerics)\n",
    "cols = ['timestamp', 'Global_active_power', 'Global_reactive_power', 'Voltage', \n",
    "        'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "data = data[cols].dropna()  # ~1.25% missing, drop for simplicity\n",
    "full_data_size = len(data)\n",
    "\n",
    "print(f\"Dataset loaded: {data.shape[0]} rows\")\n",
    "print(f\"First 5 data: {data.head()}\")\n",
    "print(f\"Data size: {full_data_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad053a51",
   "metadata": {},
   "source": [
    "### Step2: Create a Small Offline Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57655f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for sampling-based approximate SUM (scaled)\n",
    "# Aggregate column\n",
    "agg_col = 'Global_active_power'\n",
    "\n",
    "def sample_sum(query, samp_df, full_size):\n",
    "    mask = np.ones(len(samp_df), dtype=bool)\n",
    "    for dim, (lower, upper) in query.items():\n",
    "        mask &= (samp_df[dim] >= lower) & (samp_df[dim] <= upper)\n",
    "    subset_sum = samp_df.loc[mask, agg_col].sum()\n",
    "    scale = full_size / len(samp_df)\n",
    "    return subset_sum * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56d105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample created: 2049 rows\n"
     ]
    }
   ],
   "source": [
    "sample_size = int(0.001 * len(data))  # ~2000 rows\n",
    "sample = data.sample(n=sample_size, random_state=42).copy()\n",
    "print(f\"Sample created: {sample.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b378af9",
   "metadata": {},
   "source": [
    "### Step 3: Generate a Historical Query Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f0ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate avg_exact by temp queries\n",
      "Generated 2000 training queries in 2000 attempts\n",
      "Generating 2000 training queries (paper style)...\n",
      "Generated 2000 training queries in 4000 attempts\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define dimensions (7D) and their min/max\n",
    "dimensions = ['timestamp', 'Global_reactive_power', 'Voltage', 'Global_intensity', \n",
    "              'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "dim_ranges = {dim: (data[dim].min(), data[dim].max()) for dim in dimensions}\n",
    "\n",
    "def generate_random_query_paper_style():\n",
    "    \"\"\"\n",
    "    Generate query ranges exactly as described in the LAQP paper for POWER dataset:\n",
    "    - Left boundary: uniform from [min, min + (max-min)/4]  → first quarter\n",
    "    - Right boundary: uniform from [max - (max-min)/4, max] → last quarter\n",
    "    This ensures lower < upper with high probability and avoids empty results.\n",
    "    \"\"\"\n",
    "    predicates = {}\n",
    "    for dim in dimensions:\n",
    "        min_val = data[dim].min()\n",
    "        max_val = data[dim].max()\n",
    "        range_width = max_val - min_val\n",
    "        \n",
    "        # First quarter for left boundary\n",
    "        left_min = min_val\n",
    "        left_max = min_val + range_width / 4\n",
    "        lower = random.uniform(left_min, left_max)\n",
    "        \n",
    "        # Last quarter for right boundary\n",
    "        right_min = max_val - range_width / 4\n",
    "        right_max = max_val\n",
    "        upper = random.uniform(right_min, right_max)\n",
    "        \n",
    "        # Ensure lower < upper (very rare failure, but safe)\n",
    "        if lower >= upper:\n",
    "            lower, upper = right_min, right_max  # fallback\n",
    "        \n",
    "        predicates[dim] = (lower, upper)\n",
    "    \n",
    "    return predicates\n",
    "\n",
    "# Function to compute exact SUM for a query\n",
    "def exact_sum(query, df):\n",
    "    mask = np.ones(len(df), dtype=bool)\n",
    "    for dim, (lower, upper) in query.items():\n",
    "        mask &= (df[dim] >= lower) & (df[dim] <= upper)\n",
    "    return df.loc[mask, agg_col].sum()\n",
    "\n",
    "print(\"Generate avg_exact by temp queries\")\n",
    "query_log = []\n",
    "num_train_queries = 2000\n",
    "attempts = 0\n",
    "max_attempts = 20000\n",
    "while len(query_log) < num_train_queries and attempts < max_attempts:\n",
    "    q = generate_random_query_paper_style()\n",
    "    exact_result = exact_sum(q, data)\n",
    "    if exact_result > 1.0:  # safe threshold > 0\n",
    "        estimate = sample_sum(q, sample, full_data_size)\n",
    "        error = exact_result - estimate\n",
    "        query_log.append({'query': q, 'exact': exact_result, \n",
    "                          'estimate': estimate, 'error': error})\n",
    "    attempts += 1\n",
    "\n",
    "avg_exact = np.mean([query['exact'] for query in query_log])\n",
    "print(f\"Generated {len(query_log)} temp queries in {attempts} attempts\")\n",
    "print(f\"{avg_exact=}\")\n",
    "\n",
    "# For training: 2000 queries (as in paper)\n",
    "query_log = []\n",
    "\n",
    "print(\"Generating 2000 training queries (paper style)...\")\n",
    "while len(query_log) < num_train_queries and attempts < max_attempts:\n",
    "    q = generate_random_query_paper_style()\n",
    "    exact_result = exact_sum(q, data)\n",
    "    \n",
    "    if exact_result > 0.01 * avg_exact:  # safe threshold > 0\n",
    "        estimate = sample_sum(q, sample, full_data_size)\n",
    "        error = exact_result - estimate\n",
    "        query_log.append({'query': q, 'exact': exact_result, \n",
    "                          'estimate': estimate, 'error': error})\n",
    "    attempts += 1\n",
    "\n",
    "print(f\"Generated {len(query_log)} training queries in {attempts} attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e79b0",
   "metadata": {},
   "source": [
    "### Step 4: Compute Sampling-Based Estimates and Errors for the Query Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552d1120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates and errors computed for query log\n"
     ]
    }
   ],
   "source": [
    "# Add estimates and errors to query log\n",
    "for entry in query_log:\n",
    "    entry['estimate'] = sample_sum(entry['query'], sample, full_data_size)\n",
    "    entry['error'] = entry['exact'] - entry['estimate']\n",
    "\n",
    "print(\"Estimates and errors computed for query log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745dc2d",
   "metadata": {},
   "source": [
    "Diversification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_div = StandardScaler()\n",
    "\n",
    "def diversify_query_log(query_log, k=400):\n",
    "    \"\"\"\n",
    "    Diversify the query log using greedy max-min (paper Section 5.2).\n",
    "    Features: flattened ranges + error.\n",
    "    Returns a subset of k diverse entries.\n",
    "    \"\"\"\n",
    "    # Prepare features: flatten query bounds + error\n",
    "    features = []\n",
    "    for entry in query_log:\n",
    "        vec = []\n",
    "        for dim in dimensions:\n",
    "            lower, upper = entry['query'][dim]\n",
    "            vec.extend([lower, upper])\n",
    "        vec.append(entry['error'])\n",
    "        features.append(vec)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    features_norm = scaler_div.fit_transform(features)\n",
    "    \n",
    "    # Greedy max-min selection\n",
    "    selected_indices = [random.randint(0, len(features)-1)]  # Start with random\n",
    "    selected_features = features_norm[selected_indices]\n",
    "    \n",
    "    while len(selected_indices) < k:\n",
    "        # Distances from unselected to current selected set\n",
    "        dists = euclidean_distances(selected_features, features_norm)\n",
    "        min_dists = dists.min(axis=0)  # Min dist to any selected\n",
    "        # Pick the one with max min-dist (most diverse)\n",
    "        next_idx = np.argmax(min_dists)\n",
    "        selected_indices.append(next_idx)\n",
    "        selected_features = features_norm[selected_indices]\n",
    "    \n",
    "    diversified_log = [query_log[i] for i in selected_indices]\n",
    "    print(f\"Diversified query log: selected {len(diversified_log)} / {len(query_log)} entries\")\n",
    "    return diversified_log\n",
    "\n",
    "diversified_log = diversify_query_log(query_log, k=800)  # e.g., half of 800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6db18d",
   "metadata": {},
   "source": [
    "Load data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb06da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 讀取資料\n",
    "# with open('diversify_log.pkl', 'rb') as f:\n",
    "#     diversified_log = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac190cef",
   "metadata": {},
   "source": [
    "### Step 5: Train the Error Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4500066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Test MSE: 4146325.3661811887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features (flatten: lower/upper per dim) and targets (errors)\n",
    "X = []\n",
    "y = []\n",
    "# for entry in query_log:\n",
    "for entry in diversified_log:\n",
    "\n",
    "    vec = []\n",
    "    for dim in dimensions:\n",
    "        lower, upper = entry['query'][dim]\n",
    "        vec.extend([lower, upper])\n",
    "    X.append(vec)\n",
    "    y.append(entry['error'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Normalize\n",
    "X_scaled = scaler_div.fit_transform(X)\n",
    "\n",
    "# Train (80/20 split for validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "# adjust max_depth\n",
    "model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Model trained. Test MSE: {np.mean((model.predict(X_test) - y_test)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed20f77",
   "metadata": {},
   "source": [
    "### Step 6: Estimate a New Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb865d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final estimate for new query: 556459.9233206443\n"
     ]
    }
   ],
   "source": [
    "# Example new 7D query (adjust ranges to sensible values based on data mins/maxes)\n",
    "new_query = {\n",
    "    'timestamp': (0, 1e8),  # e.g., first ~few months in seconds\n",
    "    'Global_reactive_power': (0.0, 0.5),\n",
    "    'Voltage': (220, 250),\n",
    "    'Global_intensity': (0, 20),\n",
    "    'Sub_metering_1': (0, 10),\n",
    "    'Sub_metering_2': (0, 5),\n",
    "    'Sub_metering_3': (0, 15)\n",
    "}\n",
    "\n",
    "# Flatten and scale\n",
    "new_vec = []\n",
    "for dim in dimensions:\n",
    "    lower, upper = new_query[dim]\n",
    "    new_vec.extend([lower, upper])\n",
    "new_vec = np.array([new_vec])\n",
    "new_scaled = scaler_div.transform(new_vec)\n",
    "\n",
    "# Predict error\n",
    "predicted_error = model.predict(new_scaled)[0]\n",
    "\n",
    "# Find error-similar historical query (closest error)\n",
    "min_diff = float('inf')\n",
    "opt_entry = None\n",
    "for entry in diversified_log:\n",
    "    error_diff = abs(entry['error'] - predicted_error)\n",
    "    if error_diff < min_diff:\n",
    "        min_diff = error_diff\n",
    "        opt_entry = entry\n",
    "\n",
    "# Compute final estimate\n",
    "sample_new = sample_sum(new_query, sample, full_data_size)\n",
    "sample_opt = opt_entry['estimate']\n",
    "final_estimate = opt_entry['exact'] + (sample_new - sample_opt)\n",
    "\n",
    "print(f\"LAQP estimate: {final_estimate:.2f}\")\n",
    "# Compute exact for the same query (for debugging/small queries)\n",
    "exact = exact_sum(new_query, data)\n",
    "print(f\"Exact sum: {exact:.2f}\")\n",
    "print(f\"Relative error: {abs(final_estimate - exact) / exact:.4f}\")\n",
    "\n",
    "# Also see how many rows match\n",
    "mask = np.ones(len(data), dtype=bool)\n",
    "for dim, (l, u) in new_query.items():\n",
    "    mask &= (data[dim] >= l) & (data[dim] <= u)\n",
    "matched_rows = mask.sum()\n",
    "print(f\"Query matches {matched_rows:,} rows ({matched_rows / len(data):.1%} of dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3a02e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimal query index: 531 (out of  800)\n",
      "Predicted error for new query: 6852.04\n",
      "Chosen historical query error: 6844.83 (diff: 7.21)\n",
      "Exact result of chosen query: 12761.64\n",
      "Predicate ranges of chosen query:\n",
      "  timestamp: [558538.71, 99676423.59]\n",
      "  Global_reactive_power: [0.12, 1.07]\n",
      "  Voltage: [226.49, 252.27]\n",
      "  Global_intensity: [9.04, 38.87]\n",
      "  Sub_metering_1: [6.45, 85.10]\n",
      "  Sub_metering_2: [9.21, 71.13]\n",
      "  Sub_metering_3: [3.79, 23.77]\n",
      "\n",
      "Final LAQP estimate: 556459.92\n"
     ]
    }
   ],
   "source": [
    "def laqp_estimate_with_details(query):\n",
    "    # Flatten and predict error (same as before)\n",
    "    vec = []\n",
    "    for dim in dimensions:\n",
    "        l, u = query[dim]\n",
    "        vec.extend([l, u])\n",
    "    vec = np.array([vec])\n",
    "    scaled = scaler_div.transform(vec)\n",
    "    pred_error = model.predict(scaled)[0]\n",
    "    \n",
    "    # Find the most error-similar historical query\n",
    "    best_index = -1\n",
    "    best_error_diff = float('inf')\n",
    "    best_entry = None\n",
    "\n",
    "    for idx, entry in enumerate(diversified_log):\n",
    "        error_diff = abs(entry['error'] - pred_error)\n",
    "        if error_diff < best_error_diff:\n",
    "            best_error_diff = error_diff\n",
    "            best_index = idx\n",
    "            best_entry = entry\n",
    "    \n",
    "    # Compute estimates\n",
    "    sample_new = sample_sum(query, sample, full_data_size)\n",
    "    sample_opt = best_entry['estimate']\n",
    "    final_est = best_entry['exact'] + (sample_new - sample_opt)\n",
    "    \n",
    "    print(f\"Selected optimal query index: {best_index} (out of  {len(diversified_log)})\")\n",
    "    print(f\"Predicted error for new query: {pred_error:.2f}\")\n",
    "    print(f\"Chosen historical query error: {best_entry['error']:.2f} (diff: {best_error_diff:.2f})\")\n",
    "    print(f\"Exact result of chosen query: {best_entry['exact']:.2f}\")\n",
    "    print(\"Predicate ranges of chosen query:\")\n",
    "    for dim, (l, u) in best_entry['query'].items():\n",
    "        print(f\"  {dim}: [{l:.2f}, {u:.2f}]\")\n",
    "    print(f\"\\nFinal LAQP estimate: {final_est:.2f}\")    \n",
    "\n",
    "    return final_est, best_index, best_entry\n",
    "\n",
    "# Use it\n",
    "estimate, opt_idx, opt_entry = laqp_estimate_with_details(new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f582b3",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate and Extend\n",
    "Basic Evaluation: Measure Accuracy on Test Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19c042",
   "metadata": {},
   "source": [
    "Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a716d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_distance(q1, q2):\n",
    "    \"\"\"Euclidean distance on flattened predicate bounds (for range-similarity).\"\"\"\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "    for dim in dimensions:\n",
    "        l1, u1 = q1[dim]\n",
    "        l2, u2 = q2[dim]\n",
    "        vec1.extend([l1, u1])\n",
    "        vec2.extend([l2, u2])\n",
    "    return np.linalg.norm(np.array(vec1) - np.array(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8c7c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def optimize_alpha(query_log, val_queries, model, scaler, sample, full_data_size, bounds=(0,1)):\n",
    "    \"\"\"\n",
    "    Tune alpha for hybrid similarity (paper Section 5.3).\n",
    "    val_queries: List of {'query': dict, 'exact': float} for tuning.\n",
    "    Returns best alpha that minimizes average relative error on val set.\n",
    "    \"\"\"\n",
    "    def objective(alpha):\n",
    "        errors = []\n",
    "        for vq in val_queries:\n",
    "            query = vq['query']\n",
    "            exact = vq['exact']\n",
    "            \n",
    "            # Predict error\n",
    "            vec = [query[dim][i] for dim in dimensions for i in range(2)]\n",
    "            vec = np.array([vec])\n",
    "            scaled = scaler_div.transform(vec)\n",
    "            pred_error = model.predict(scaled)[0]\n",
    "            \n",
    "            # Find best entry with hybrid similarity\n",
    "            best_entry = min(diversified_log, key=lambda e: \n",
    "                alpha * abs(e['error'] - pred_error) + \n",
    "                (1 - alpha) * range_distance(query, e['query']))\n",
    "            \n",
    "            # LAQP estimate\n",
    "            sample_new = sample_sum(query, sample, full_data_size)\n",
    "            sample_opt = best_entry['estimate']\n",
    "            laqp_est = best_entry['exact'] + (sample_new - sample_opt)\n",
    "            \n",
    "            # Relative error\n",
    "            rel_err = abs(laqp_est - exact) / (exact + 1e-6)\n",
    "            errors.append(rel_err)\n",
    "        \n",
    "        return np.mean(errors)\n",
    "    \n",
    "    # Optimize alpha\n",
    "    res = minimize_scalar(objective, bounds=bounds, method='bounded')\n",
    "    best_alpha = res.x\n",
    "    print(f\"Optimized alpha: {best_alpha:.3f} (MSE on val: {res.fun:.4f})\")\n",
    "    return best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dbe9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100 test queries...\n",
      "Generated 100 test queries\n",
      "Optimized alpha: 0.998 (MSE on val: 0.1367)\n",
      "Using minimum exact threshold: 1000.00 (avg exact = 11769.49)\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 separate test queries\n",
    "test_queries = []\n",
    "num_test = 100\n",
    "\n",
    "print(\"Generating 100 test queries...\")\n",
    "while len(test_queries) < num_test:\n",
    "    q = generate_random_query_paper_style()\n",
    "    exact_result = exact_sum(q, data)\n",
    "    if exact_result > 0.01 * avg_exact:\n",
    "        test_queries.append({'query': q, 'exact': exact_result})\n",
    "\n",
    "print(f\"Generated {len(test_queries)} test queries\")\n",
    "\n",
    "val_queries = test_queries[:50]\n",
    "final_test_queries = test_queries[50:]\n",
    "best_alpha = optimize_alpha(diversified_log, val_queries, model, scaler_div, sample, full_data_size)\n",
    "\n",
    "# Evaluation lists\n",
    "laqp_rel_errors = []\n",
    "sampling_rel_errors = []\n",
    "laqp_abs_errors = []\n",
    "sampling_abs_errors = []\n",
    "\n",
    "# Threshold for filtering tiny queries (adjustable)\n",
    "# Good starting values: 1000 or 0.01 * average exact sum\n",
    "avg_exact = np.mean([tq['exact'] for tq in final_test_queries])\n",
    "min_exact_threshold = max(1000.0, 0.01 * avg_exact)  # at least 1000 or 1% of avg\n",
    "\n",
    "print(f\"Using minimum exact threshold: {min_exact_threshold:.2f} \"\n",
    "      f\"(avg exact = {avg_exact:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b10c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Evaluated on 50 / 50 queries (excluded 0 tiny ones)\n",
      "\n",
      "LAQP    Average Relative Error (ARE): 0.1457\n",
      "LAQP    Median Relative Error:       0.0831\n",
      "LAQP    Mean Absolute Error (MAE):     1607.78\n",
      "\n",
      "Sampling ARE:                       0.6657\n",
      "Sampling Median Relative Error:     0.5734\n",
      "Sampling MAE:                       6537.78\n",
      "\n",
      "Improvement (Sampling ARE / LAQP ARE): 4.57x\n"
     ]
    }
   ],
   "source": [
    "filtered_count = 0\n",
    "\n",
    "for tq in final_test_queries:\n",
    "    query = tq['query']\n",
    "    exact = tq['exact']\n",
    "    \n",
    "    if exact < min_exact_threshold:\n",
    "        continue  # skip tiny queries that distort relative error\n",
    "    filtered_count += 1\n",
    "    \n",
    "    # --- Pure Sampling Estimate ---\n",
    "    sample_est = sample_sum(query, sample, full_data_size)\n",
    "    sampling_abs = abs(sample_est - exact)\n",
    "    sampling_rel = sampling_abs / exact\n",
    "    \n",
    "    sampling_abs_errors.append(sampling_abs)\n",
    "    sampling_rel_errors.append(sampling_rel)\n",
    "    \n",
    "    # --- LAQP Estimate ---\n",
    "    # Flatten and predict error\n",
    "    vec = []\n",
    "    for dim in dimensions:\n",
    "        l, u = query[dim]\n",
    "        vec.extend([l, u])\n",
    "    vec = np.array([vec])\n",
    "    scaled = scaler_div.transform(vec)\n",
    "    predicted_error = model.predict(scaled)[0]\n",
    "    \n",
    "    # Hybrid selection\n",
    "    best_entry = min(diversified_log, key=lambda e: \n",
    "        best_alpha * abs(e['error'] - predicted_error) + \n",
    "        (1 - best_alpha) * range_distance(query, e['query']))\n",
    "    \n",
    "    # Compute estimates\n",
    "    sample_new = sample_sum(query, sample, full_data_size)\n",
    "    sample_opt = best_entry['estimate']\n",
    "    laqp_est = best_entry['exact'] + (sample_new - sample_opt)\n",
    "    \n",
    "    laqp_abs = abs(laqp_est - exact)\n",
    "    laqp_rel = laqp_abs / exact\n",
    "    \n",
    "    laqp_abs_errors.append(laqp_abs)\n",
    "    laqp_rel_errors.append(laqp_rel)\n",
    "\n",
    "# ========================\n",
    "# Results\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Evaluated on {filtered_count} / {len(final_test_queries)} queries \"\n",
    "      f\"(excluded {len(final_test_queries)-filtered_count} tiny ones)\")\n",
    "\n",
    "if len(laqp_rel_errors) > 0:\n",
    "    print(f\"\\nLAQP    Average Relative Error (ARE): {np.mean(laqp_rel_errors):.4f}\")\n",
    "    print(f\"LAQP    Median Relative Error:       {np.median(laqp_rel_errors):.4f}\")\n",
    "    print(f\"LAQP    Mean Absolute Error (MAE):     {np.mean(laqp_abs_errors):.2f}\")\n",
    "    \n",
    "    print(f\"\\nSampling ARE:                       {np.mean(sampling_rel_errors):.4f}\")\n",
    "    print(f\"Sampling Median Relative Error:     {np.median(sampling_rel_errors):.4f}\")\n",
    "    print(f\"Sampling MAE:                       {np.mean(sampling_abs_errors):.2f}\")\n",
    "    \n",
    "    improvement = np.mean(sampling_rel_errors) / np.mean(laqp_rel_errors)\n",
    "    print(f\"\\nImprovement (Sampling ARE / LAQP ARE): {improvement:.2f}x\")\n",
    "else:\n",
    "    print(\"No queries passed the threshold — try lowering min_exact_threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e2668",
   "metadata": {},
   "source": [
    "### Store data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 儲存資料 (原本的 data_list)\n",
    "with open('data_storage.pkl', 'wb') as f:\n",
    "    pickle.dump(diversified_log, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
